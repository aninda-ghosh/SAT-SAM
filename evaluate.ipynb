{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rpn.build_rpn import RPN_Model\n",
    "from sam.build_sam import SAM_Model\n",
    "from transformers import pipeline\n",
    "\n",
    "from data_builder.build_dataset import PlanetscopeDataset\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from eval_utils import filter_boxes, calculate_iou, calculate_precision_recall\n",
    "\n",
    "import pandas as pd\n",
    "from rpn import _transforms as T\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(image_enhancement=\"FALSE\"):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    if image_enhancement == \"TRUE\":\n",
    "        transforms.append(T.ContrastBasedAdaptiveGammaCorrection())\n",
    "    transforms.append(T.ToTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Print the evaluation config from the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a folder with unix timestamp\n",
    "root_path = os.getcwd() + '/results/' + str(int(time.time())) + '/'\n",
    "os.mkdir(root_path)\n",
    "\n",
    "with open('eval_config.json', 'r') as f:\n",
    "    eval_config = json.load(f)\n",
    "\n",
    "\n",
    "print(\"Evaluation Configuration\")\n",
    "print(json.dumps(eval_config, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the device and load the dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "dataset1_test = PlanetscopeDataset(eval_config['DATASET1_PATH'], get_transform(image_enhancement=eval_config['IMAGE_ENHANCEMENT']))\n",
    "dataset2_test = PlanetscopeDataset(eval_config['DATASET2_PATH'], get_transform(image_enhancement=eval_config['IMAGE_ENHANCEMENT']))\n",
    "\n",
    "dataset_test = ConcatDataset([dataset1_test, dataset2_test])\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset_test)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-int(len(indices)*0.2):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the proper model to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_config['MODEL'] == 'SAT-SAM':\n",
    "    rpn_model = RPN_Model(eval_config['RPN_MODEL_PATH'], 2, device, eval_config['TRAIN_TYPE'])\n",
    "    sam_model = SAM_Model(eval_config['SAM_MODEL_PATH'], 'large', device)\n",
    "elif eval_config['MODEL'] == 'SAM':\n",
    "    vanilla_sam_model = pipeline(\"mask-generation\", model=\"facebook/sam-vit-large\", device=device)\n",
    "elif eval_config['MODEL'] == 'MASKRCNN':\n",
    "    maskrcnn_model = RPN_Model(eval_config['RPN_MODEL_PATH'], 2, device, eval_config['TRAIN_TYPE']) #Load the Pre-Trained MaskRCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['parcel_id', 'parcel_path', 'gt_mask_ct', 'pred_mask_ct', 'mean_iou', 'p_50', 'r_50', 'p_70', 'r_70', 'p_90', 'r_90'])\n",
    "\n",
    "for i, (id, sam_image, rpn_image, target, ensemble, path)  in enumerate(dataset_test): \n",
    "    try:\n",
    "        if eval_config['MODEL'] == 'SAT-SAM':\n",
    "            print('SAT-SAM')\n",
    "            rpn_image = rpn_image.squeeze(0).to(device)  \n",
    "            predictions = rpn_model.predict(rpn_image)\n",
    "            predictions = rpn_model.postprocess(predictions, nms_threshold=eval_config['NMS_THRESHOLD'], score_threshold=eval_config['PRED_CONFIDENCE_THRESHOLD'])\n",
    "            \n",
    "            if eval_config['FILTRATION'] == 'TRUE':\n",
    "                filtered_predictions = filter_boxes(predictions, ensemble, eval_config['ENSEMBLE_BOX_BETA'], eval_config['ENSEMBLE_BOX_OVERLAP_THRESHOLD'])\n",
    "                low_res_masks, iou_predictions = sam_model.predict(sam_image, filtered_predictions)\n",
    "            else:\n",
    "                low_res_masks, iou_predictions = sam_model.predict(sam_image, predictions['boxes'])\n",
    "            \n",
    "            high_res_masks = sam_model.postprocess(low_res_masks, tuple(sam_image.size))\n",
    "            pred_masks = high_res_masks.squeeze().cpu().numpy()\n",
    "        \n",
    "        elif eval_config['MODEL'] == 'SAM':\n",
    "            print('SAM')\n",
    "            outputs = vanilla_sam_model(sam_image, points_per_batch=32)\n",
    "            pred_masks = outputs[\"masks\"]\n",
    "        \n",
    "        elif eval_config['MODEL'] == 'MASKRCNN':\n",
    "            print('MASKRCNN')\n",
    "            rpn_image = rpn_image.squeeze(0).to(device)\n",
    "            predictions = maskrcnn_model.predict(rpn_image)\n",
    "            predictions = maskrcnn_model.postprocess(predictions, nms_threshold=eval_config['NMS_THRESHOLD'], score_threshold=eval_config['PRED_CONFIDENCE_THRESHOLD'])\n",
    "            pred_masks = predictions['masks']\n",
    "\n",
    "        iou_score, iou_matrix = calculate_iou(target_masks=np.array(target['masks']), predicted_masks=np.array(pred_masks))\n",
    "        \n",
    "        p_50, r_50 = calculate_precision_recall(iou_matrix, len(pred_masks), len(target['masks']), threshold=0.5)\n",
    "        p_70, r_70 = calculate_precision_recall(iou_matrix, len(pred_masks), len(target['masks']), threshold=0.7)\n",
    "        p_90, r_90 = calculate_precision_recall(iou_matrix, len(pred_masks), len(target['masks']), threshold=0.9)\n",
    "        \n",
    "        # Round off the values to 2 decimal places\n",
    "        iou_score = round(iou_score, 2)\n",
    "        p_50, r_50 = round(p_50, 2), round(r_50, 2)\n",
    "        p_70, r_70 = round(p_70, 2), round(r_70, 2)\n",
    "        p_90, r_90 = round(p_90, 2), round(r_90, 2)\n",
    "\n",
    "        print(\"Image Id: \", id, \" Average IoU Score: \", iou_score)\n",
    "        print(\"IoU Matrix: \", iou_matrix)\n",
    "        print(\"Precision: \", p_50, p_70, p_90)\n",
    "        print(\"Recall: \", r_50, r_70, r_90)\n",
    "\n",
    "        results.loc[i] = [id, path, len(target['masks']), len(pred_masks), iou_score, p_50, r_50, p_70, r_70, p_90, r_90]\n",
    "    except:\n",
    "        print(\"Error in image: \", id)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_IoU = results['mean_iou'].mean()\n",
    "eval_config['MEAN_IOU'] = mean_IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(root_path + 'results', index=False)\n",
    "\n",
    "# save model and configuration\n",
    "with open(root_path + 'eval_config.json', 'w') as f:\n",
    "    json.dump(eval_config, f, indent=1)\n",
    "    f.close()\n",
    "print(\"Evaluation Configuration saved to \" + root_path + 'train_config.json') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
