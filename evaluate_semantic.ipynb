{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghosh57/.conda/envs/dl4cv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rpn.build_rpn import RPN_Model\n",
    "from sam.build_sam import SAM_Model\n",
    "from transformers import pipeline\n",
    "\n",
    "from data_builder.build_dataset import PlanetscopeDataset\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from eval_utils import filter_boxes_predictions, filter_boxes, calculate_iou, calculate_precision_recall, filter_masks\n",
    "from eval_utils import generate_random_bounding_boxes, generate_uniform_spaced_bounding_boxes\n",
    "\n",
    "import pandas as pd\n",
    "from rpn import _transforms as T\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(image_enhancement=\"FALSE\"):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    if image_enhancement == \"TRUE\":\n",
    "        transforms.append(T.ContrastBasedAdaptiveGammaCorrection())\n",
    "    transforms.append(T.ToTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Print the evaluation config from the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Configuration\n",
      "{\n",
      " \"DATASET_PATH\": \"/scratch/aghosh57/SAT-SAM(Dataset)/ps_rwanda/all_dataset/\",\n",
      " \"DATASET\": \"RWANDA\",\n",
      " \"MODEL\": \"SAT-SAM\",\n",
      " \"IMAGE_ENHANCEMENT\": \"FALSE\",\n",
      " \"FILTRATION\": \"FALSE\",\n",
      " \"TRAIN_TYPE\": \"FINETUNE\",\n",
      " \"NMS_THRESHOLD\": 0.9,\n",
      " \"PRED_CONFIDENCE_THRESHOLD\": 0.6,\n",
      " \"ENSEMBLE_BOX_OVERLAP_THRESHOLD\": 0.5,\n",
      " \"ENSEMBLE_BOX_BETA\": 0.4,\n",
      " \"RPN_MODEL_PATH\": \"rpn/checkpoints/1692043128/rpn_model_1.09.pth\",\n",
      " \"SAM_MODEL_PATH\": \"sam/checkpoint/sam_vit_l_0b3195.pth\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Create a folder with unix timestamp\n",
    "root_path = os.getcwd() + '/results/' + str(int(time.time())) + '/'\n",
    "os.mkdir(root_path)\n",
    "\n",
    "with open('eval_config.json', 'r') as f:\n",
    "    eval_config = json.load(f)\n",
    "\n",
    "\n",
    "print(\"Evaluation Configuration\")\n",
    "print(json.dumps(eval_config, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the device and load the dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "dataset_test = PlanetscopeDataset(eval_config['DATASET_PATH'], get_transform(image_enhancement=eval_config['IMAGE_ENHANCEMENT']))\n",
    "# dataset2_test = PlanetscopeDataset(eval_config['DATASET2_PATH'], get_transform(image_enhancement=eval_config['IMAGE_ENHANCEMENT']))\n",
    "\n",
    "# dataset_test = ConcatDataset([dataset1_test, dataset2_test])\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset_test)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-int(len(indices)*0.2):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the proper model to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_config['MODEL'] == 'SAT-SAM':\n",
    "    rpn_model = RPN_Model(eval_config['RPN_MODEL_PATH'], 2, device, eval_config['TRAIN_TYPE'])\n",
    "    sam_model = SAM_Model(eval_config['SAM_MODEL_PATH'], 'large', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(predicted, target):\n",
    "    correct_pixels = np.sum(predicted == target)\n",
    "    total_pixels = predicted.shape[0] * predicted.shape[1]\n",
    "    accuracy = correct_pixels / total_pixels\n",
    "    return accuracy\n",
    "\n",
    "def f1_score(predicted, target):\n",
    "    true_positive = np.sum(np.logical_and(predicted == 1, target == 1))\n",
    "    false_positive = np.sum(np.logical_and(predicted == 1, target == 0))\n",
    "    false_negative = np.sum(np.logical_and(predicted == 0, target == 1))\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive + 1e-8)\n",
    "    recall = true_positive / (true_positive + false_negative + 1e-8)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    return f1\n",
    "\n",
    "def mean_iou(predicted, target):\n",
    "    intersection = np.logical_and(predicted, target)\n",
    "    union = np.logical_or(predicted, target)\n",
    "    \n",
    "    intersection_area = np.sum(intersection)\n",
    "    union_area = np.sum(union)\n",
    "    \n",
    "    iou = intersection_area / (union_area + 1e-8)\n",
    "    return iou\n",
    "\n",
    "def precision_at_iou_threshold(predicted, target, iou_threshold):\n",
    "    true_positive = np.sum(np.logical_and(predicted == 1, target == 1))\n",
    "    false_positive = np.sum(np.logical_and(predicted == 1, target == 0))\n",
    "    \n",
    "    iou = true_positive / (true_positive + false_positive + 1e-8)\n",
    "    \n",
    "    precision = 0.0\n",
    "    if iou >= iou_threshold:\n",
    "        precision = iou\n",
    "        \n",
    "    return precision\n",
    "\n",
    "def collate_mask(masks, img_size):\n",
    "    cumulative_mask = np.zeros(img_size, dtype=np.uint8)\n",
    "    \n",
    "    for mask in masks:\n",
    "        cumulative_mask[mask > 0] = 1\n",
    "    \n",
    "    return cumulative_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Id:  42\n",
      "Pixel Accuracy:  0.9134695870535714\n",
      "F1 Score:  0.7099360262693679\n",
      "mIoU:  0.5503107198341403\n",
      "Image Id:  14\n",
      "Pixel Accuracy:  0.5663265306122449\n",
      "F1 Score:  0.16817981568043147\n",
      "mIoU:  0.0918102233954766\n",
      "Image Id:  30\n",
      "Pixel Accuracy:  0.5502580915178571\n",
      "F1 Score:  0.18054886877146611\n",
      "mIoU:  0.09923260385792701\n",
      "Image Id:  18\n",
      "Pixel Accuracy:  0.8181002869897959\n",
      "F1 Score:  0.39793528311545684\n",
      "mIoU:  0.24838902270803773\n",
      "Image Id:  67\n",
      "Pixel Accuracy:  0.8996482382015306\n",
      "F1 Score:  0.7689960840271529\n",
      "mIoU:  0.6246902077703113\n",
      "Image Id:  71\n",
      "Pixel Accuracy:  0.3054149394132653\n",
      "F1 Score:  0.04305385706828124\n",
      "mIoU:  0.02200053317618512\n",
      "Image Id:  68\n",
      "Pixel Accuracy:  0.5900231186224489\n",
      "F1 Score:  0.5155889367677166\n",
      "mIoU:  0.3473356917365716\n",
      "Image Id:  27\n",
      "Pixel Accuracy:  0.903693997130102\n",
      "F1 Score:  0.24226743132632592\n",
      "mIoU:  0.13782951960384593\n",
      "Image Id:  60\n",
      "Pixel Accuracy:  0.6626275510204082\n",
      "F1 Score:  0.43623132492405226\n",
      "mIoU:  0.27896154788143\n",
      "Image Id:  51\n",
      "Pixel Accuracy:  0.8760214046556123\n",
      "F1 Score:  0.7784100472900294\n",
      "mIoU:  0.6372105907738034\n",
      "Image Id:  29\n",
      "Pixel Accuracy:  0.7440310108418368\n",
      "F1 Score:  0.5126916023057142\n",
      "mIoU:  0.34471102947737287\n",
      "Image Id:  56\n",
      "Pixel Accuracy:  0.6789750079719388\n",
      "F1 Score:  0.0542516164063878\n",
      "mIoU:  0.02788213461276907\n",
      "Image Id:  93\n",
      "Pixel Accuracy:  0.9189453125\n",
      "F1 Score:  0.4152828662536454\n",
      "mIoU:  0.262054887729525\n",
      "Image Id:  47\n",
      "Pixel Accuracy:  0.6924426020408163\n",
      "F1 Score:  0.5653753491103626\n",
      "mIoU:  0.39409287670422233\n",
      "Image Id:  97\n",
      "Pixel Accuracy:  0.9631895727040817\n",
      "F1 Score:  0.7754816701675735\n",
      "mIoU:  0.633295279693933\n",
      "Image Id:  94\n",
      "Pixel Accuracy:  0.5645577566964286\n",
      "F1 Score:  0.12680967108173535\n",
      "mIoU:  0.06769716559455652\n",
      "Image Id:  86\n",
      "Pixel Accuracy:  0.9241719148596939\n",
      "F1 Score:  0.0\n",
      "mIoU:  0.0\n",
      "Image Id:  87\n",
      "Pixel Accuracy:  0.8198092713647959\n",
      "F1 Score:  0.3611891211784038\n",
      "mIoU:  0.2203970768931815\n",
      "Image Id:  65\n",
      "Pixel Accuracy:  0.6021305006377551\n",
      "F1 Score:  0.42363656863397176\n",
      "mIoU:  0.26874296022927735\n",
      "Image Id:  53\n",
      "Pixel Accuracy:  0.7957987882653061\n",
      "F1 Score:  0.4663680583359692\n",
      "mIoU:  0.3040938651452118\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['parcel_id', 'parcel_path', 'gt_mask_ct', 'pred_mask_ct', 'pixel_acc', 'F1_score', 'mIoU'])\n",
    "    \n",
    "for i, (id, sam_image, rpn_image, target, ensemble, path)  in enumerate(dataset_test): \n",
    "    \n",
    "    rpn_image = rpn_image.squeeze(0).to(device)  \n",
    "    predictions = rpn_model.predict(rpn_image)\n",
    "    predictions = rpn_model.postprocess(predictions, nms_threshold=eval_config['NMS_THRESHOLD'], score_threshold=eval_config['PRED_CONFIDENCE_THRESHOLD'])\n",
    "    \n",
    "    if predictions['boxes'].shape[0] == 0:\n",
    "        print(\"No boxes detected, generating uniform boxes\")\n",
    "        bboxes = generate_uniform_spaced_bounding_boxes(sam_image.size[0], sam_image.size[1], int(sam_image.size[0]*0.25))\n",
    "        if eval_config['FILTRATION'] == 'TRUE':\n",
    "            filtered_boxes = filter_boxes(bboxes, ensemble, eval_config['ENSEMBLE_BOX_OVERLAP_THRESHOLD'])\n",
    "            low_res_masks, iou_predictions = sam_model.predict(sam_image, filtered_boxes)\n",
    "        else:\n",
    "            low_res_masks, iou_predictions = sam_model.predict(sam_image, bboxes)\n",
    "    else:\n",
    "        if eval_config['FILTRATION'] == 'TRUE':\n",
    "            filtered_predictions = filter_boxes_predictions(predictions, ensemble, eval_config['ENSEMBLE_BOX_BETA'], eval_config['ENSEMBLE_BOX_OVERLAP_THRESHOLD'])\n",
    "            low_res_masks, iou_predictions = sam_model.predict(sam_image, filtered_predictions)\n",
    "        else:\n",
    "            low_res_masks, iou_predictions = sam_model.predict(sam_image, predictions['boxes'])\n",
    "    \n",
    "    high_res_masks = sam_model.postprocess(low_res_masks, tuple(sam_image.size))\n",
    "    pred_masks = high_res_masks.squeeze().cpu().numpy()\n",
    "\n",
    "    # Stack the target masks to get a single mask\n",
    "    pred_masks = collate_mask(pred_masks, sam_image.size)\n",
    "    target_masks = collate_mask(target['masks'], sam_image.size)\n",
    "\n",
    "    pix_acc = pixel_accuracy(pred_masks, target_masks)\n",
    "    f1_sc = f1_score(pred_masks, target_masks) \n",
    "    mIoU = mean_iou(pred_masks, target_masks)\n",
    "    # precision_95 = precision_at_iou_threshold(pred_masks, target_masks, 0.95)\n",
    "\n",
    "    print(\"Image Id: \", id)\n",
    "    print(\"Pixel Accuracy: \", pix_acc)\n",
    "    print(\"F1 Score: \", f1_sc)\n",
    "    print(\"mIoU: \", mIoU)\n",
    "    # print(\"Precision @ 0.95: \", precision_95)\n",
    "\n",
    "    results.loc[i] = [id, path, len(target['masks']), len(pred_masks), pix_acc, f1_sc, mIoU]\n",
    "    # except:\n",
    "    #     print(\"Error in image: \", id)\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config['MEAN_IOU'] = results['mIoU'].mean()\n",
    "eval_config['MEAN_PIXEL_ACCURACY'] = results['pixel_acc'].mean()\n",
    "eval_config['MEAN_F1_SCORE'] = results['F1_score'].mean()\n",
    "# eval_config['MEAN_PRECISION@0.95'] = results['precision@0.95'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Configuration saved to /home/aghosh57/Kerner-Lab/SAT-SAM/results/1692519291/train_config.json\n"
     ]
    }
   ],
   "source": [
    "results.to_csv(root_path + 'results', index=False)\n",
    "\n",
    "# save model and configuration\n",
    "with open(root_path + 'eval_config.json', 'w') as f:\n",
    "    json.dump(eval_config, f, indent=1)\n",
    "    f.close()\n",
    "print(\"Evaluation Configuration saved to \" + root_path + 'train_config.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
